#include "internal.h"

//------------------------------------------------------------------------------
template<typename T>
inline void veda_tensors_adadelta(T* _var, T* _accum, T* _accum_update, const T* _grad, const T rho, const T epsilon, const T lr, const size_t numel) {
	veda_omp_simd(numel, [&](const size_t min, const size_t max) {
		auto cnt			= max - min;
		auto var			= _var + min;
		auto accum			= _accum + min;
		auto accum_update	= _accum_update + min;
		auto grad			= _grad + min;

		for(auto i = 0; i < cnt; i++) {
			auto rho1	= static_cast<T>(1) - rho;
			auto grad_	= grad[i];
			auto accum_	= accum[i] * rho + (grad_ * grad_) * rho1;
			accum[i]	= accum_;

			auto accum_update_	 = accum_update[i];
			const T update		 = std::sqrt(accum_update_ + epsilon) / std::sqrt(accum_ + epsilon) * grad[i];
			var[i]				-= update * lr;
			accum_update[i]		 = accum_update_ * rho + (update * update) * rho1;
		}
	}, veda_omp_vlen<T>());
}

//------------------------------------------------------------------------------
template<typename T>
inline void veda_tensors_adadelta(void* var, void* accum, void* accum_update, const void* grad, const uint64_t* rho, const uint64_t* epsilon, const uint64_t* lr, const size_t numel) {
	veda_tensors_adadelta((T*)var, (T*)accum, (T*)accum_update, (const T*)grad, *(const T*)rho, *(const T*)epsilon, *(const T*)lr, numel);
}

//------------------------------------------------------------------------------
__global__ void veda_tensors_adadelta(ATTR_PTR(var), ATTR_PTR(accum), ATTR_PTR(accum_update), ATTR_PTR(grad), ATTR_SCALAR(rho), ATTR_SCALAR(epsilon), ATTR_SCALAR(lr), const size_t numel, const int dtype) {
	DEREF_PTR(var);
	DEREF_PTR(accum);
	DEREF_PTR(accum_update);
	DEREF_PTR(grad);

	DEREF_SCALAR(rho);
	DEREF_SCALAR(epsilon);
	DEREF_SCALAR(lr);

	KERNEL_FLOAT(dtype, veda_tensors_adadelta, var, accum, accum_update, grad, rho, epsilon, lr, numel);
}

//------------------------------------------------------------------------------
